#!/usr/bin/env python3
"""
DAG Visualizer for nanoGPT DAG-augmented model.

This module provides visualization capabilities for the DAG computation graphs
generated by the DAGPlanPredictor and DifferentiableDAG components.

Usage:
    from dag_model import GPT, GPTConfig
    from dag_visualizer import DAGVisualizer

    # Create model with DAG components
    config = GPTConfig(dag_depth=3, dag_scratch_nodes=2, ...)
    model = GPT(config)

    # Create visualizer
    visualizer = DAGVisualizer(config)

    # Analyze and visualize
    input_tokens = torch.randint(0, config.vocab_size, (batch_size, seq_len))
    analysis = visualizer.analyze_model_dag(model, input_tokens, batch_idx=0)
    trace = analysis['trace']

    # Generate visualizations
    visualizer.visualize_batch_overview(trace)              # Overall statistics
    visualizer.visualize_token_dag(trace, token_idx=0)      # Single token DAG
    visualizer.visualize_sequence_dag_evolution(trace)      # Sequence evolution

Features:
    - Extract DAG execution traces from DAGPlanPredictor
    - Visualize individual token computation graphs
    - Show operation usage patterns across sequences
    - Display node access frequency heatmaps
    - Track DAG complexity evolution
    - Support probability display and graph customization
"""

import math
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union

import matplotlib.patches as patches
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import torch

# Add project root to path for imports
sys.path.insert(0, str(Path(__file__).resolve().parent))
from dag_model import (GPT, DAGPlanPredictor, DifferentiableDAG, GPTConfig,
                       op_funcs, op_names)


class DAGVisualizer:
    """
    Visualizes DAG computation graphs from DAGPlanPredictor and DifferentiableDAG.

    Can produce:
    - Individual token DAG evolution graphs
    - Batch overview graphs
    - Operation frequency analysis
    - Node utilization heatmaps
    """

    def __init__(self, config: GPTConfig):
        self.config = config
        self.dag_depth = config.dag_depth
        self.scratch_nodes = config.dag_scratch_nodes
        self.op_names = op_names
        self.n_ops = len(op_funcs)

        # Color schemes
        self.operation_colors = {
            "add": "#FF6B6B",
            "identity": "#4ECDC4",
            "multiply": "#45B7D1",
            "subtract": "#96CEB4",
            "divide": "#FECA57",
            "power": "#FF9FF3",
            "log": "#54A0FF",
            "max_op": "#5F27CD",
            "min_op": "#00D2D3",
        }

        self.node_colors = {
            "input": "#E8F4FD",
            "computation": "#FFE4E1",
            "final": "#E8F5E8",
        }

    def extract_dag_execution_trace(
        self,
        plan_predictor: DAGPlanPredictor,
        hidden_states: torch.Tensor,
        batch_idx: int = 0,
    ) -> Dict:
        """
        Extract the DAG execution trace for a specific batch sample.

        Args:
            plan_predictor: The DAGPlanPredictor to analyze
            hidden_states: Input hidden states (B, T, H)
            batch_idx: Which batch sample to analyze

        Returns:
            Dictionary containing execution trace information
        """
        with torch.no_grad():
            # Get the DAG plan
            operand1_probs, operand2_probs, operation_probs = plan_predictor(
                hidden_states
            )

            B, T, H = hidden_states.shape

            # Extract data for specific batch
            op1_probs = operand1_probs[batch_idx]  # (T, dag_depth, max_nodes)
            op2_probs = operand2_probs[batch_idx]  # (T, dag_depth, max_nodes)
            op_probs = operation_probs[batch_idx]  # (T, dag_depth, n_ops)

            trace = {
                "sequence_length": T,
                "dag_depth": self.dag_depth,
                "scratch_nodes": self.scratch_nodes,
                "tokens": [],
                "global_stats": {
                    "operation_usage": torch.zeros(self.n_ops),
                    "node_access_frequency": torch.zeros(T, self.scratch_nodes),
                    "step_complexity": torch.zeros(self.dag_depth),
                },
            }

            # Process each token
            for t in range(T):
                token_info = {
                    "position": t,
                    "available_nodes": (t + 1) * self.scratch_nodes,
                    "steps": [],
                }

                # Process each DAG step for this token
                for step in range(self.dag_depth):
                    # Get most likely operands and operation
                    available_nodes = (t + 1) * self.scratch_nodes

                    if available_nodes > 0:
                        op1_probs_step = op1_probs[t, step, :available_nodes]
                        op2_probs_step = op2_probs[t, step, :available_nodes]
                        op_probs_step = op_probs[t, step]

                        # Find most likely choices
                        op1_choice = torch.argmax(op1_probs_step).item()
                        op2_choice = torch.argmax(op2_probs_step).item()
                        op_choice = torch.argmax(op_probs_step).item()

                        # Convert flat node indices to (token, slot) format
                        op1_token = op1_choice // self.scratch_nodes
                        op1_slot = op1_choice % self.scratch_nodes
                        op2_token = op2_choice // self.scratch_nodes
                        op2_slot = op2_choice % self.scratch_nodes

                        step_info = {
                            "step": step,
                            "operand1": {
                                "flat_index": op1_choice,
                                "token": op1_token,
                                "slot": op1_slot,
                                "prob": op1_probs_step[op1_choice].item(),
                            },
                            "operand2": {
                                "flat_index": op2_choice,
                                "token": op2_token,
                                "slot": op2_slot,
                                "prob": op2_probs_step[op2_choice].item(),
                            },
                            "operation": {
                                "index": op_choice,
                                "name": self.op_names[op_choice],
                                "prob": op_probs_step[op_choice].item(),
                            },
                            "target_slot": step % self.scratch_nodes,  # FIFO target
                        }

                        # Update global statistics
                        trace["global_stats"]["operation_usage"][op_choice] += 1
                        trace["global_stats"]["node_access_frequency"][
                            op1_token, op1_slot
                        ] += 1
                        trace["global_stats"]["node_access_frequency"][
                            op2_token, op2_slot
                        ] += 1
                        # Calculate entropy manually: -sum(p * log(p))
                        entropy = (
                            -(op_probs_step * torch.log(op_probs_step + 1e-10))
                            .sum()
                            .item()
                        )
                        trace["global_stats"]["step_complexity"][step] += entropy

                        token_info["steps"].append(step_info)

                trace["tokens"].append(token_info)

            return trace

    def visualize_token_dag(
        self,
        trace: Dict,
        token_idx: int,
        save_path: Optional[str] = None,
        show_probabilities: bool = True,
    ):
        """
        Visualize the DAG computation for a specific token.

        Args:
            trace: Execution trace from extract_dag_execution_trace
            token_idx: Which token to visualize
            save_path: Optional path to save the figure
            show_probabilities: Whether to show probability values
        """
        if token_idx >= len(trace["tokens"]):
            raise ValueError(
                f"Token index {token_idx} out of range (max: {len(trace['tokens'])-1})"
            )

        token_info = trace["tokens"][token_idx]
        fig, ax = plt.subplots(1, 1, figsize=(14, 10))

        G = nx.DiGraph()
        pos = {}
        node_colors = []
        node_labels = {}
        edge_labels = {}

        # Calculate layout parameters
        max_steps = len(token_info["steps"])
        available_tokens = token_idx + 1

        # Add initial nodes (input nodes from all previous and current tokens)
        y_input = 0
        for prev_token in range(available_tokens):
            for slot in range(self.scratch_nodes):
                node_id = f"t{prev_token}_s{slot}_input"
                G.add_node(node_id)
                pos[node_id] = (prev_token * 2 + slot * 0.5, y_input)
                node_colors.append(self.node_colors["input"])
                node_labels[node_id] = f"T{prev_token}S{slot}"

        # Add computation nodes for each step
        for step_idx, step in enumerate(token_info["steps"]):
            y_step = -(step_idx + 1) * 2

            # Create computation node
            comp_node_id = f"t{token_idx}_step{step['step']}"
            G.add_node(comp_node_id)
            pos[comp_node_id] = (token_idx * 2 + step["target_slot"] * 0.5, y_step)
            node_colors.append(
                self.operation_colors.get(step["operation"]["name"], "#CCCCCC")
            )

            if show_probabilities:
                node_labels[comp_node_id] = (
                    f"{step['operation']['name']}\n({step['operation']['prob']:.2f})"
                )
            else:
                node_labels[comp_node_id] = step["operation"]["name"]

            # Add edges from operands to computation
            op1_source = (
                f"t{step['operand1']['token']}_s{step['operand1']['slot']}_input"
            )
            op2_source = (
                f"t{step['operand2']['token']}_s{step['operand2']['slot']}_input"
            )

            # Update source nodes if they're from previous computations
            for prev_step_idx in range(step_idx):
                prev_step = token_info["steps"][prev_step_idx]
                if (
                    step["operand1"]["token"] == token_idx
                    and step["operand1"]["slot"] == prev_step["target_slot"]
                ):
                    op1_source = f"t{token_idx}_step{prev_step['step']}"
                if (
                    step["operand2"]["token"] == token_idx
                    and step["operand2"]["slot"] == prev_step["target_slot"]
                ):
                    op2_source = f"t{token_idx}_step{prev_step['step']}"

            G.add_edge(op1_source, comp_node_id)
            G.add_edge(op2_source, comp_node_id)

            if show_probabilities:
                edge_labels[(op1_source, comp_node_id)] = (
                    f"op1 ({step['operand1']['prob']:.2f})"
                )
                edge_labels[(op2_source, comp_node_id)] = (
                    f"op2 ({step['operand2']['prob']:.2f})"
                )

        # Draw the graph
        nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=1000, ax=ax)
        nx.draw_networkx_labels(G, pos, labels=node_labels, font_size=8, ax=ax)
        nx.draw_networkx_edges(
            G, pos, edge_color="gray", arrows=True, arrowsize=20, arrowstyle="->", ax=ax
        )

        if show_probabilities and edge_labels:
            nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=6, ax=ax)

        ax.set_title(
            f"DAG Computation for Token {token_idx}\n"
            f"Available nodes: {token_info['available_nodes']}, "
            f"Computation steps: {len(token_info['steps'])}",
            fontsize=14,
            fontweight="bold",
        )
        ax.axis("off")

        # Add legend
        legend_elements = []
        for op_name, color in self.operation_colors.items():
            if op_name in [step["operation"]["name"] for step in token_info["steps"]]:
                legend_elements.append(patches.Patch(color=color, label=op_name))

        if legend_elements:
            ax.legend(
                handles=legend_elements, loc="upper right", bbox_to_anchor=(1.15, 1)
            )

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches="tight")

        plt.show()

    def visualize_batch_overview(self, trace: Dict, save_path: Optional[str] = None):
        """
        Create an overview visualization of the entire batch DAG execution.

        Args:
            trace: Execution trace from extract_dag_execution_trace
            save_path: Optional path to save the figure
        """
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))

        # 1. Operation usage distribution
        op_usage = trace["global_stats"]["operation_usage"].numpy()
        colors = [
            self.operation_colors.get(op_name, "#CCCCCC") for op_name in self.op_names
        ]

        bars = ax1.bar(self.op_names, op_usage, color=colors)
        ax1.set_title("Operation Usage Distribution", fontweight="bold")
        ax1.set_ylabel("Usage Count")
        ax1.tick_params(axis="x", rotation=45)

        # Add value labels on bars
        for bar, value in zip(bars, op_usage):
            if value > 0:
                ax1.text(
                    bar.get_x() + bar.get_width() / 2,
                    bar.get_height() + 0.1,
                    f"{int(value)}",
                    ha="center",
                    va="bottom",
                )

        # 2. Node access frequency heatmap
        node_freq = trace["global_stats"]["node_access_frequency"].numpy()
        im = ax2.imshow(node_freq.T, cmap="Blues", aspect="auto")
        ax2.set_title("Node Access Frequency Heatmap", fontweight="bold")
        ax2.set_xlabel("Token Position")
        ax2.set_ylabel("Scratch Slot")
        ax2.set_yticks(range(self.scratch_nodes))
        plt.colorbar(im, ax=ax2, label="Access Count")

        # 3. DAG complexity by step
        step_complexity = trace["global_stats"]["step_complexity"].numpy()
        ax3.plot(
            range(self.dag_depth), step_complexity, "o-", linewidth=2, markersize=8
        )
        ax3.set_title("DAG Complexity by Step", fontweight="bold")
        ax3.set_xlabel("DAG Step")
        ax3.set_ylabel("Average Entropy")
        ax3.grid(True, alpha=0.3)

        # 4. Token DAG size progression
        token_dag_sizes = [len(token["steps"]) for token in trace["tokens"]]
        ax4.plot(
            range(trace["sequence_length"]),
            token_dag_sizes,
            "s-",
            linewidth=2,
            markersize=6,
            color="purple",
        )
        ax4.set_title("DAG Steps per Token", fontweight="bold")
        ax4.set_xlabel("Token Position")
        ax4.set_ylabel("Number of DAG Steps")
        ax4.grid(True, alpha=0.3)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches="tight")

        plt.show()

    def visualize_sequence_dag_evolution(
        self, trace: Dict, save_path: Optional[str] = None
    ):
        """
        Show how the DAG evolves across the sequence.

        Args:
            trace: Execution trace from extract_dag_execution_trace
            save_path: Optional path to save the figure
        """
        T = trace["sequence_length"]
        fig, ax = plt.subplots(1, 1, figsize=(max(12, T * 2), 8))

        G = nx.DiGraph()
        pos = {}
        node_colors = []
        node_labels = {}

        # Layout: x-axis is token position, y-axis is time/step
        for t in range(T):
            token_info = trace["tokens"][t]

            # Add initial input nodes
            for slot in range(self.scratch_nodes):
                node_id = f"t{t}_s{slot}_init"
                G.add_node(node_id)
                pos[node_id] = (t, slot * 0.5)
                node_colors.append(self.node_colors["input"])
                node_labels[node_id] = f"T{t}S{slot}"

            # Add computation nodes
            for step_info in token_info["steps"]:
                step = step_info["step"]
                node_id = f"t{t}_step{step}"
                G.add_node(node_id)
                pos[node_id] = (t, -(step + 1))
                color = self.operation_colors.get(
                    step_info["operation"]["name"], "#CCCCCC"
                )
                node_colors.append(color)
                node_labels[node_id] = step_info["operation"]["name"]

                # Add edges showing data flow
                # This is simplified - in reality we'd need to track which exact nodes are accessed
                source_t = step_info["operand1"]["token"]
                if source_t < t:  # Cross-token dependency
                    source_node = f"t{source_t}_s{step_info['operand1']['slot']}_init"
                    if source_node in G.nodes():
                        G.add_edge(source_node, node_id)

        # Draw the graph
        nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=600, ax=ax)
        nx.draw_networkx_labels(G, pos, labels=node_labels, font_size=7, ax=ax)
        nx.draw_networkx_edges(
            G, pos, edge_color="gray", arrows=True, arrowsize=15, alpha=0.6, ax=ax
        )

        ax.set_title("DAG Evolution Across Sequence", fontsize=16, fontweight="bold")
        ax.set_xlabel("Token Position")
        ax.set_ylabel("Computation Depth (negative = deeper)")
        ax.grid(True, alpha=0.3)

        # Add operation legend
        legend_elements = []
        used_ops = set()
        for token in trace["tokens"]:
            for step in token["steps"]:
                used_ops.add(step["operation"]["name"])

        for op_name in used_ops:
            color = self.operation_colors.get(op_name, "#CCCCCC")
            legend_elements.append(patches.Patch(color=color, label=op_name))

        if legend_elements:
            ax.legend(
                handles=legend_elements, loc="upper left", bbox_to_anchor=(1.02, 1)
            )

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches="tight")

        plt.show()

    def analyze_model_dag(
        self,
        model: GPT,
        input_tokens: torch.Tensor,
        batch_idx: int = 0,
        layer_idx: Optional[int] = None,
    ) -> Dict:
        """
        Analyze DAG execution for a complete model forward pass.

        Args:
            model: The GPT model with DAG components
            input_tokens: Input token tensor (B, T)
            batch_idx: Which batch sample to analyze
            layer_idx: Which layer to analyze (None for final layer)

        Returns:
            Analysis results dictionary
        """
        model.eval()

        with torch.no_grad():
            # Get hidden states at the specified layer
            if layer_idx is None:
                hidden_states = model.forward_hidden(input_tokens)
            else:
                # Forward through specific layers
                x = input_tokens
                device = x.device
                b, t = x.size()

                # Token and position embeddings
                tok_emb = model.transformer.wte(x)
                pos = torch.arange(0, t, dtype=torch.long, device=device)
                pos_emb = model.transformer.wpe(pos)
                x = model.transformer.drop(tok_emb + pos_emb)

                # Forward through layers up to layer_idx
                for i, block in enumerate(model.transformer.h):
                    x = block(x)
                    if i == layer_idx:
                        break

                hidden_states = x

            # Extract DAG execution trace
            if hasattr(model, "dag") and model.dag is not None:
                trace = self.extract_dag_execution_trace(
                    model.dag.plan_predictor, hidden_states, batch_idx
                )

                return {
                    "trace": trace,
                    "hidden_states": hidden_states,
                    "model_config": model.config,
                }
            else:
                raise ValueError("Model does not have DAG components")


def demo_dag_visualization():
    """
    Demonstration of DAG visualization capabilities.
    """
    print("DAG Visualization Demo")
    print("=" * 40)

    # Create a small model for demonstration
    config = GPTConfig(
        vocab_size=50,
        block_size=8,
        n_layer=2,
        n_head=4,
        n_embd=32,
        dag_depth=3,
        dag_scratch_nodes=2,
    )

    model = GPT(config)
    visualizer = DAGVisualizer(config)

    # Create sample input
    batch_size = 2
    seq_len = 5
    input_tokens = torch.randint(0, config.vocab_size, (batch_size, seq_len))

    print(f"Analyzing model with {model.get_num_params():,} parameters")
    print(f"Input shape: {input_tokens.shape}")

    # Analyze DAG execution
    analysis = visualizer.analyze_model_dag(model, input_tokens, batch_idx=0)
    trace = analysis["trace"]

    print(f"Extracted trace for {trace['sequence_length']} tokens")
    print(
        f"Total DAG steps across sequence: {sum(len(t['steps']) for t in trace['tokens'])}"
    )

    # Create visualizations
    print("\nGenerating visualizations...")

    # 1. Batch overview
    visualizer.visualize_batch_overview(trace)

    # 2. Individual token DAGs
    for token_idx in range(min(3, trace["sequence_length"])):
        if trace["tokens"][token_idx]["steps"]:  # Only if token has DAG steps
            visualizer.visualize_token_dag(trace, token_idx)

    # 3. Sequence evolution
    visualizer.visualize_sequence_dag_evolution(trace)

    print("Demo completed!")


if __name__ == "__main__":
    demo_dag_visualization()
