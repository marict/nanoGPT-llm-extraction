#!/usr/bin/env python3
"""
Script to pull down RunPod checkpoint and test it on example DAGs.

This script:
1. Pulls down the specified checkpoint from RunPod volume
2. Saves it locally to runpod_checkpoints/
3. Tests the checkpoint on example DAGs like 1.0 + 1.0
"""

import shutil
import sys
import tempfile
from pathlib import Path

# Add the project root to the path
sys.path.append(str(Path(__file__).parent.parent))

import torch
import torch.nn as nn

from checkpoint_manager import CheckpointManager
from data.nalm.streaming import NALMDataset, create_nalm_dataloaders
from models.dag_model import GPT, GPTConfig
from models.predictor_only_model import PredictorOnlyConfig, PredictorOnlyModel


def pull_checkpoint_from_runpod(source_path: str, local_dir: Path) -> Path:
    """Pull checkpoint from RunPod volume to local directory."""
    checkpoint_manager = CheckpointManager()
    return checkpoint_manager.download_checkpoint_from_runpod(source_path, local_dir)


def load_checkpoint_and_model(checkpoint_path: Path):
    """Load checkpoint and create model."""
    print(f"üîÑ Loading checkpoint from {checkpoint_path}...")

    checkpoint_manager = CheckpointManager()

    try:
        # Load checkpoint
        checkpoint = checkpoint_manager.load_checkpoint_from_path(checkpoint_path)
        print(f"‚úÖ Checkpoint loaded successfully")

        # Extract model args
        model_args = checkpoint["model_args"]
        print(f"üìä Model configuration:")
        print(f"   - DAG depth: {model_args.dag_depth}")
        print(f"   - Max digits: {model_args.max_digits}")
        print(f"   - Max decimal places: {model_args.max_decimal_places}")
        print(f"   - Layers: {model_args.n_layer}")
        print(f"   - Heads: {model_args.n_head}")
        print(f"   - Embedding dim: {model_args.n_embd}")

        # Create model
        if hasattr(model_args, "dag_depth") and model_args.dag_depth > 0:
            model = PredictorOnlyModel(model_args)
            model_type = "DAG Predictor"
        else:
            model = GPT(model_args)
            model_type = "DAG-GPT"

        # Load state dict
        model.load_state_dict(checkpoint["model"])
        print(f"‚úÖ {model_type} model created and loaded")

        # Get parameter count
        total_params = sum(p.numel() for p in model.parameters())
        print(f"üìä Model parameters: {total_params:,}")

        return model, model_args, checkpoint

    except Exception as e:
        print(f"‚ùå Error loading checkpoint: {e}")
        return None, None, None


def test_simple_dag_examples(model, model_args):
    """Test the model on simple DAG examples."""
    print(f"\nüß™ Testing simple DAG examples...")

    model.eval()

    # Test 1: Simple arithmetic expressions
    print(f"üìù Test 1: Simple arithmetic expressions")

    # Create NALM dataset with simple examples
    dataset = NALMDataset(
        split="train",
        operations=["add", "sub", "mul", "div"],
        train_range=(-2.0, 2.0),
        num_examples=5,
        seed=42,
    )

    for i in range(3):
        item = dataset[i]
        expression = item["expression"]
        a, b = item["a"], item["b"]
        expected_result = item["result"]

        print(f"   Example {i+1}: {expression}")
        print(f"     - Input: a={a:.2f}, b={b:.2f}")
        print(f"     - Expected: {expected_result:.6f}")

        # For now, we'll just show the expression since we need to tokenize it
        # In a full implementation, you'd tokenize the expression and run it through the model
        print(f"     - Model would process: {expression}")

    # Test 2: Specific 1.0 + 1.0 case
    print(f"\nüìù Test 2: Specific 1.0 + 1.0 case")

    # Create a dataset that's more likely to generate 1.0 + 1.0
    simple_dataset = NALMDataset(
        split="train",
        operations=["add"],
        train_range=(0.9, 1.1),  # Range around 1.0
        num_examples=10,
        seed=42,
    )

    found_simple = False
    for i in range(10):
        item = simple_dataset[i]
        if abs(item["a"] - 1.0) < 0.1 and abs(item["b"] - 1.0) < 0.1:
            print(f"   Found simple case: {item['expression']}")
            print(f"     - a={item['a']:.6f}, b={item['b']:.6f}")
            print(f"     - Expected result: {item['result']:.6f}")
            found_simple = True
            break

    if not found_simple:
        print(f"   No exact 1.0 + 1.0 found, showing closest:")
        item = simple_dataset[0]
        print(f"   Closest: {item['expression']}")
        print(f"     - a={item['a']:.6f}, b={item['b']:.6f}")
        print(f"     - Expected result: {item['result']:.6f}")


def test_model_forward_pass(model, model_args):
    """Test the model's forward pass on sample inputs."""
    print(f"\nüß™ Testing model forward pass...")

    model.eval()

    # Test input
    test_input = torch.randint(0, 1000, (2, 10))
    print(f"üìù Test input shape: {test_input.shape}")

    with torch.no_grad():
        if isinstance(model, PredictorOnlyModel):
            # DAG Predictor model
            outputs = model(test_input)
            digit_logits, V_sign, O, G = outputs

            print(f"üìä DAG Predictor outputs:")
            print(f"   - Digit logits: {digit_logits.shape}")
            print(f"   - V_sign: {V_sign.shape}")
            print(f"   - O: {O.shape}")
            print(f"   - G: {G.shape}")

            # Show some sample values
            print(f"   - Sample V_sign values: {V_sign[0, :3].tolist()}")
            print(f"   - Sample G values: {G[0, 0, :].tolist()}")

        else:
            # DAG-GPT model
            dag_results = model(test_input)

            print(f"üìä DAG-GPT outputs:")
            print(f"   - Output shape: {dag_results.shape}")
            print(f"   - Sample values: {dag_results[0, :5].tolist()}")

    print(f"‚úÖ Forward pass successful")


def compare_with_fresh_model(model, model_args):
    """Compare loaded model with fresh initialization."""
    print(f"\nüß™ Comparing with fresh model...")

    # Create fresh model with same config
    torch.manual_seed(123)  # Different seed
    if isinstance(model, PredictorOnlyModel):
        fresh_model = PredictorOnlyModel(model_args)
    else:
        fresh_model = GPT(model_args)

    fresh_model.eval()
    model.eval()

    # Test input
    test_input = torch.randint(0, 1000, (2, 10))

    with torch.no_grad():
        if isinstance(model, PredictorOnlyModel):
            # Compare DAG predictor outputs
            loaded_outputs = model(test_input)
            fresh_outputs = fresh_model(test_input)

            digit_logits_loaded, V_sign_loaded, O_loaded, G_loaded = loaded_outputs
            digit_logits_fresh, V_sign_fresh, O_fresh, G_fresh = fresh_outputs

            # Calculate differences
            digit_diff = (
                torch.abs(digit_logits_loaded - digit_logits_fresh).mean().item()
            )
            vsign_diff = torch.abs(V_sign_loaded - V_sign_fresh).mean().item()
            o_diff = torch.abs(O_loaded - O_fresh).mean().item()
            g_diff = torch.abs(G_loaded - G_fresh).mean().item()

            print(f"üìä Output differences (loaded vs fresh):")
            print(f"   - Digit logits: {digit_diff:.6f}")
            print(f"   - V_sign: {vsign_diff:.6f}")
            print(f"   - O: {o_diff:.6f}")
            print(f"   - G: {g_diff:.6f}")

        else:
            # Compare DAG-GPT outputs
            loaded_results = model(test_input)
            fresh_results = fresh_model(test_input)

            diff = torch.abs(loaded_results - fresh_results).mean().item()
            print(f"üìä DAG-GPT output difference: {diff:.6f}")

    print(f"‚úÖ Comparison complete")


def main():
    """Main function to pull checkpoint and run tests."""
    print("üöÄ RunPod Checkpoint Testing Script")
    print("=" * 50)

    # Configuration
    runpod_checkpoint_path = "/runpod-volume/checkpoints/rhfuok9btu6t8m-pretrained_sharp/ckpt_predictor_pretrain_8600_99.98acc.pt"
    local_checkpoint_dir = Path("runpod_checkpoints")

    # Step 1: Pull checkpoint from RunPod
    local_checkpoint_path = pull_checkpoint_from_runpod(
        runpod_checkpoint_path, local_checkpoint_dir
    )
    if local_checkpoint_path is None:
        print("‚ùå Failed to pull checkpoint. Exiting.")
        return

    # Step 2: Load checkpoint and model
    model, model_args, checkpoint = load_checkpoint_and_model(local_checkpoint_path)
    if model is None:
        print("‚ùå Failed to load model. Exiting.")
        return

    # Step 3: Test simple DAG examples
    test_simple_dag_examples(model, model_args)

    # Step 4: Test model forward pass
    test_model_forward_pass(model, model_args)

    # Step 5: Compare with fresh model
    compare_with_fresh_model(model, model_args)

    print(f"\n‚úÖ All tests completed successfully!")
    print(f"üìÅ Checkpoint saved locally at: {local_checkpoint_path}")


if __name__ == "__main__":
    main()
